from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
import requests
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
from langchain.schema import Document
from typing import List
import hashlib
import json
import redis
from datetime import datetime
import time

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve static files
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def read_index():
    return FileResponse('static/index.html')

def make_lm_studio_request(url: str, payload: dict, timeout: int = 120, max_retries: int = 2):
    """Make request to LM Studio with retry logic and better timeout handling"""
    for attempt in range(max_retries + 1):
        try:
            print(f"üîó LM Studio request attempt {attempt + 1}/{max_retries + 1} (timeout: {timeout}s)")
            response = requests.post(
                url,
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=timeout
            )
            if response.status_code == 200:
                return response
            else:
                print(f"‚ö†Ô∏è LM Studio returned status {response.status_code}: {response.text}")
                
        except requests.exceptions.Timeout:
            print(f"‚è∞ Timeout on attempt {attempt + 1} (waited {timeout}s)")
            if attempt < max_retries:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"üîÑ Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                raise Exception(f"LM Studio timeout after {max_retries + 1} attempts")
                
        except requests.exceptions.ConnectionError:
            raise Exception("LM Studio server not responding - please check if it's running")
        except Exception as e:
            print(f"‚ùå LM Studio error on attempt {attempt + 1}: {str(e)}")
            if attempt == max_retries:
                raise

class NomicEmbedding(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            payload = {"model": "text-embedding-nomic-embed-text-v1.5", "input": texts}
            response = make_lm_studio_request(
                "http://127.0.0.1:1234/v1/embeddings", 
                payload, 
                timeout=30  # Shorter timeout for embeddings
            )
            return [d["embedding"] for d in response.json()["data"]]
        except Exception as e:
            print(f"‚ùå Embedding failed: {str(e)}")
            raise Exception(f"Failed to generate embeddings: {str(e)}")

    def embed_query(self, text: str) -> List[float]:
        return self.embed_documents([text])[0]

embedding = NomicEmbedding()
vectorstore = Chroma(persist_directory="./chroma_data", embedding_function=embedding)

# Initialize semantic cache (separate Chroma instance)
semantic_cache = Chroma(
    persist_directory="./semantic_cache_data", 
    embedding_function=embedding,
    collection_name="rag_responses"
)

# Initialize document cache
try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    redis_client.ping()  # Test connection
    print("‚úÖ Redis connected for document caching")
except:
    print("‚ö†Ô∏è  Redis not available, document cache disabled")
    redis_client = None

# Configuration
SEMANTIC_SIMILARITY_THRESHOLD = 0.7  # Much higher threshold - identical questions scoring 0.655!
DOCUMENT_CACHE_TTL = 3600  # 1 hour
SEMANTIC_CACHE_TTL = 7 * 24 * 3600  # 7 days

@app.post("/query")
async def query_rag(request: Request):
    body = await request.json()
    question = body.get("question")
    print(f"Received question: {question}")
    print(f"\nüîç Processing query: '{question}'")
    print(f"üìä Cache threshold: {SEMANTIC_SIMILARITY_THRESHOLD}")
    
    # === SEMANTIC CACHE CHECK ===
    print("üß† Checking semantic cache...")
    try:
        cache_results = semantic_cache.similarity_search_with_score(question, k=3)  # Get top 3 for debugging
        print(f"üì¶ Found {len(cache_results)} cached items")
        
        if cache_results:
            for i, (doc, score) in enumerate(cache_results):
                cached_question = doc.metadata.get('question', 'N/A')
                print(f"   #{i+1}: Score {score:.3f} - Question: {cached_question[:50]}...")
                
                # Debug: Check if this is an exact match
                if cached_question.lower().strip() == question.lower().strip():
                    print(f"   üéØ EXACT TEXT MATCH found but similarity score is {score:.3f} (should be ~0.0)")
                    print(f"   üí° This suggests embeddings/ChromaDB issue - forcing cache hit!")
                    
                    # Force cache hit for exact text matches regardless of similarity score
                    try:
                        cached_response = json.loads(doc.page_content)
                        cached_response["cache_hit"] = True
                        cached_response["cache_type"] = "exact_match"
                        cached_response["similarity_score"] = score
                        cached_response["cached_question"] = cached_question
                        
                        print(f"üöÄ FORCED cache hit for exact match!")
                        return cached_response
                    except json.JSONDecodeError as json_error:
                        print(f"‚ùå Failed to parse exact match cached response: {json_error}")
            
            # Check best match
            best_doc, best_score = cache_results[0]
            print(f"üéØ Best match score: {best_score:.3f} (threshold: {SEMANTIC_SIMILARITY_THRESHOLD})")
            print(f"üîç Similarity check: {best_score:.3f} < {SEMANTIC_SIMILARITY_THRESHOLD} = {best_score < SEMANTIC_SIMILARITY_THRESHOLD}")
            
            if best_score < SEMANTIC_SIMILARITY_THRESHOLD:
                print(f"‚úÖ SEMANTIC CACHE HIT! Using cached response (similarity: {best_score:.3f})")
                print(f"üöÄ Cache hit means NO LLM processing needed - instant response!")
                
                # Parse cached response
                try:
                    cached_response = json.loads(best_doc.page_content)
                    cached_response["cache_hit"] = True
                    cached_response["cache_type"] = "semantic"
                    cached_response["similarity_score"] = best_score
                    cached_response["cached_question"] = best_doc.metadata.get('question', '')
                    
                    print(f"üöÄ Returning cached response in ~50ms instead of 10+ seconds!")
                    return cached_response
                except json.JSONDecodeError as json_error:
                    print(f"‚ùå Failed to parse cached response JSON: {json_error}")
            else:
                print(f"‚ùå No cache hit - best score {best_score:.3f} > threshold {SEMANTIC_SIMILARITY_THRESHOLD}")
                print(f"üí° Identical questions should score ~0.0, not {best_score:.3f}")
                print(f"üí° Consider increasing threshold or checking embedding consistency")
        else:
            print("üì≠ No cached items found - this is a new question")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Semantic cache error: {e}")
        import traceback
        traceback.print_exc()
    
    print("üîÑ Cache miss - proceeding with full RAG pipeline...")
    
    # === DOCUMENT CACHE CHECK ===
    doc_cache_key = f"docs:{hashlib.md5(question.encode()).hexdigest()}"
    relevant_docs = None
    
    print(f"üìÑ Checking document cache with key: {doc_cache_key[:20]}...")
    if redis_client:
        try:
            cached_docs_json = redis_client.get(doc_cache_key)
            if cached_docs_json:
                print("üìÑ Document cache HIT!")
                cached_docs_data = json.loads(cached_docs_json)
                relevant_docs = [
                    type('Document', (), {
                        'page_content': doc['content'],
                        'metadata': doc['metadata']
                    })() 
                    for doc in cached_docs_data
                ]
                print(f"üìÑ Loaded {len(relevant_docs)} cached documents")
            else:
                print("üìÑ Document cache MISS")
        except Exception as e:
            print(f"‚ö†Ô∏è Document cache error: {e}")
    else:
        print("üìÑ Redis not available - document cache disabled")
    
    # === FULL VECTOR SEARCH (if no document cache hit) ===
    if relevant_docs is None:
        print("üîç Performing full vector search...")
        results = vectorstore.similarity_search_with_score(question, k=10)
        relevant_docs = [doc for doc, score in results if score < 1]
        
        # Cache the documents
        if redis_client and relevant_docs:
            try:
                docs_to_cache = [
                    {
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    }
                    for doc in relevant_docs
                ]
                redis_client.setex(
                    doc_cache_key, 
                    DOCUMENT_CACHE_TTL, 
                    json.dumps(docs_to_cache)
                )
                print(f"üìÑ Cached {len(relevant_docs)} documents")
            except Exception as e:
                print(f"‚ö†Ô∏è Document cache save error: {e}")
    
    if not relevant_docs:
        return {
            "answer": "I couldn't find relevant information in the ArcGIS Enterprise documentation.",
            "reasoning": "No relevant documents found in the knowledge base.",
            "raw_response": "",
            "used_granite": False,
            "cache_hit": False
        }
    
    # === LLM PROCESSING ===
    context = "\n\n".join([doc.page_content for doc in relevant_docs])
    
    try:
        print(f"Received question: {question}")
        
        # Step 1: Get detailed response with reasoning from DeepSeek-R1
        deepseek_payload = {
            "model": "deepseek-r1-distill-qwen-7b",
            "messages": [
                {"role": "system", "content": "You are an ArcGIS Enterprise expert. Think through the problem step by step and provide a detailed technical answer based on the context."},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}"}
            ],
            "temperature": 0.7,
            "max_tokens": -1,
            "stream": False
        }

        print("Step 1: Sending request to DeepSeek-R1...")
        deepseek_response = make_lm_studio_request(
            "http://127.0.0.1:1234/v1/chat/completions",
            deepseek_payload,
            timeout=180  # 3 minutes for complex reasoning
        )
        
        # deepseek_response will only be returned if successful due to make_lm_studio_request retry logic

        deepseek_result = deepseek_response.json()
        raw_answer = deepseek_result['choices'][0]['message']['content']
        print(f"DeepSeek response length: {len(raw_answer)} characters")
        
        # Extract reasoning and final answer
        reasoning = ""
        final_answer = raw_answer
        use_granite = False
        
        # Look for thinking tags in the response
        if "<think>" in raw_answer and "</think>" in raw_answer:
            print("Found <think> tags - extracting reasoning...")
            think_start = raw_answer.find("<think>")
            think_end = raw_answer.find("</think>")
            reasoning = raw_answer[think_start+7:think_end].strip()
            final_answer = raw_answer[think_end+8:].strip()
            print(f"Extracted reasoning ({len(reasoning)} chars) and answer ({len(final_answer)} chars)")
            
            # Check if final answer is well-formatted
            if not any(marker in final_answer for marker in ["##", "**", "-", "*", "`", "1."]):
                print("Final answer lacks formatting - will use Granite for cleanup")
                use_granite = True
        else:
            print("No <think> tags found - will use Granite for formatting")
            use_granite = True

        # Step 2: Only use Granite if the answer needs formatting cleanup
        if final_answer.strip() and use_granite:
            print("Step 2: Sending to Granite for formatting cleanup...")
            granite_payload = {
                "model": "granite-3.1-8b-instruct:2",
                "messages": [
                    {
                        "role": "system", 
                        "content": "You are a technical documentation specialist. Take the provided response and create a clean, well-formatted markdown summary. Use headings, bullet points, code blocks, and clear structure. Be concise but comprehensive."
                    },
                    {
                        "role": "user", 
                        "content": f"Please format this ArcGIS Enterprise response into clean markdown:\n\n{final_answer}"
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 1024,
                "stream": False
            }
            
            granite_response = make_lm_studio_request(
                "http://127.0.0.1:1234/v1/chat/completions",
                granite_payload,
                timeout=90  # 1.5 minutes for formatting
            )
            
            # granite_response will only be returned if successful due to make_lm_studio_request retry logic
            granite_result = granite_response.json()
            summarized_answer = granite_result['choices'][0]['message']['content']
            print(f"Granite formatted response length: {len(summarized_answer)} characters")
            
            # === CACHE THE GRANITE RESPONSE ===
            print("üíæ Caching Granite-formatted response...")
            response_to_cache = {
                "answer": summarized_answer,
                "reasoning": reasoning if reasoning else None,
                "raw_response": final_answer,
                "used_granite": True,
                "cache_hit": False,
                "cached_at": datetime.now().isoformat()
            }
            
            # Store in semantic cache
            try:
                cache_document = Document(
                    page_content=json.dumps(response_to_cache),
                    metadata={
                        "question": question,
                        "timestamp": datetime.now().isoformat(),
                        "doc_count": len(relevant_docs),
                        "response_type": "granite"
                    }
                )
                semantic_cache.add_documents([cache_document])
                print(f"‚úÖ Successfully cached Granite response for question: '{question[:50]}...'")
                print(f"üìä Cache now contains response for future similar queries")
            except Exception as cache_error:
                print(f"‚ùå Failed to cache Granite response: {cache_error}")
                import traceback
                traceback.print_exc()
            
            return {
                "answer": summarized_answer,
                "reasoning": reasoning if reasoning else None,
                "raw_response": final_answer,
                "used_granite": True,
                "cache_hit": False
            }
        else:
            print(f"Granite formatting skipped - using DeepSeek response directly")
        
        # === CACHE THE DEEPSEEK RESPONSE (when Granite not used or failed) ===
        print("üíæ Caching DeepSeek response...")
        response_to_cache = {
            "answer": final_answer,
            "reasoning": reasoning if reasoning else None,
            "raw_response": raw_answer,
            "used_granite": False,
            "cache_hit": False,
            "cached_at": datetime.now().isoformat()
        }
        
        # Store in semantic cache
        try:
            cache_document = Document(
                page_content=json.dumps(response_to_cache),
                metadata={
                    "question": question,
                    "timestamp": datetime.now().isoformat(),
                    "doc_count": len(relevant_docs),
                    "response_type": "deepseek"
                }
            )
            semantic_cache.add_documents([cache_document])
            print(f"‚úÖ Successfully cached DeepSeek response for question: '{question[:50]}...'")
            print(f"üìä Cache now contains response for future similar queries")
        except Exception as cache_error:
            print(f"‚ùå Failed to cache DeepSeek response: {cache_error}")
            import traceback
            traceback.print_exc()
        
        # Use DeepSeek response directly
        return {
            "answer": final_answer,
            "reasoning": reasoning if reasoning else None,
            "raw_response": raw_answer,
            "used_granite": False,
            "cache_hit": False
        }
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error in query_rag: {error_msg}")
        import traceback
        traceback.print_exc()
        
        # Provide user-friendly error messages
        if "timeout" in error_msg.lower():
            return {
                "error": "‚è∞ The AI model is taking longer than expected to respond. This can happen with complex questions. Please try:\n\n" +
                        "‚Ä¢ **Simplify your question** - Break complex queries into smaller parts\n" +
                        "‚Ä¢ **Check LM Studio** - Ensure models are loaded and GPU/RAM is sufficient\n" +
                        "‚Ä¢ **Try again** - The system will retry automatically\n" +
                        "‚Ä¢ **Restart LM Studio** if issues persist\n\n" +
                        "The system supports up to 3-minute processing for complex reasoning.",
                "timeout": True,
                "technical_error": error_msg
            }
        elif "connection" in error_msg.lower():
            return {
                "error": "üîó Cannot connect to LM Studio. Please ensure:\n\n" +
                        "‚Ä¢ **LM Studio is running** on port 1234\n" +
                        "‚Ä¢ **Models are loaded** (DeepSeek-R1, Granite, Nomic)\n" +
                        "‚Ä¢ **Local server is started** in LM Studio\n" +
                        "‚Ä¢ **No firewall blocking** localhost connections",
                "connection_error": True,
                "technical_error": error_msg
            }
        else:
            return {
                "error": f"Server error: {error_msg}",
                "technical_error": error_msg
            }

# Add a debug endpoint to check cache contents
@app.get("/debug/cache")
async def debug_cache():
    try:
        # Get all cached items
        cache_results = semantic_cache.similarity_search("", k=10)
        cache_info = []
        
        for doc in cache_results:
            try:
                cached_data = json.loads(doc.page_content)
                cache_info.append({
                    "question": doc.metadata.get('question', 'N/A')[:100],
                    "timestamp": doc.metadata.get('timestamp', 'N/A'),
                    "response_type": doc.metadata.get('response_type', 'unknown'),
                    "answer_preview": cached_data.get('answer', 'N/A')[:100] + "..." if cached_data.get('answer') else 'N/A'
                })
            except:
                cache_info.append({
                    "question": doc.metadata.get('question', 'N/A'),
                    "error": "Failed to parse cached response"
                })
        
        return {
            "cache_count": len(cache_info),
            "cached_items": cache_info,
            "threshold": SEMANTIC_SIMILARITY_THRESHOLD
        }
    except Exception as e:
        return {"error": f"Failed to debug cache: {str(e)}"}
